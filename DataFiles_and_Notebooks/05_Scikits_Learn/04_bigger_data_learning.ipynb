{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While sklearn can be parallelized and extremely versitile in the breath of algorithms it exposes it is not built for \"medium\"/\"big\" data. Some algorithms are \"incremental\" in the sense that they can be updated with only a small window on the training data (see http://scikit-learn.org/stable/modules/scaling_strategies.html). But for many learning algorithms, data is usually kept in RAM (and potentially copied many times) so you typically cannot learn on data with featurized size $>$ 10% RAM.\n",
    "\n",
    "- `dask-learn` is a new project that is attempting to expose much of sklearn capabilities but atop `dask.distributed` and hence do a lot of computation out of core (see https://github.com/dask/dask-learn). \n",
    "\n",
    "- `graphlab` (aka turi) is a Python 2.7 project for out-of-core learning and data-science pipeling (https://github.com/turi-code).\n",
    "\n",
    "- `MLLib` sits atop Spark and is meant for large-scale distributed learning tasks (http://spark.apache.org/docs/latest/ml-guide.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py3k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
